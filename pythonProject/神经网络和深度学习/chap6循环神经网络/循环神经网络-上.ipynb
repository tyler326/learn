{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 第6章 循环神经网络\n",
    "\n",
    "循环神经网络（Recurrent Neural Network，RNN）是一类具有短期记忆能力的神经网络．在循环神经网络中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构．和前馈神经网络相比，循环神经网络更加符合生物神经网络的结构．目前，循环神经网络已经被广泛应用在语音识别、语言模型以及自然语言生成等任务上．\n",
    "\n",
    "本章内容基于《神经网络与深度学习》第6章：循环神经网络的相关内容进行设计。在阅读本章之前，建议先了解如图6.1所示的关键知识点，以便更好地理解和掌握相应的理论和实践知识。\n",
    "\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/1ec5f69f253b4419bcfe1f1f515c08f34758f85ded7e49fb8b3457065016a7b3\" width=500></center>\n",
    "<br><center>图6.1 《神经网络与深度学习》关键知识点回顾</center></br>\n",
    "\n",
    "\n",
    "本章内容主要包含两部分：\n",
    "\n",
    "- 模型解读：介绍经典循环神经网络原理，为了更好地理解长程依赖问题，我们设计一个简单的数字求和任务来验证简单循环网络的记忆能力。长程依赖问题具体可分为梯度爆炸和梯度消失两种情况。对于梯度爆炸，我们复现简单循环网络的梯度爆炸现象并尝试解决。对于梯度消失，一种有效的方式是改进模型，我们也动手实现一个长短期记忆网络，并观察是否可以缓解长程依赖问题。\n",
    "- 案例实践：基于双向长短期记忆网络实现文本分类任务．并了解如何进行补齐序列数据，如何将文本数据转为向量表示，如何对补齐位置进行掩蔽等实践知识。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "循环神经网络非常擅于处理序列数据，通过使用带自反馈的神经元，能够处理任意长度的序列数据．给定输入序列$[\\boldsymbol{x}_0, \\boldsymbol{x}_1, \\boldsymbol{x}_2, ...]$，循环神经网络从左到右扫描该序列，并不断调用一个相同的组合函数$f(\\cdot)$来处理时序信息．这个函数也称为循环神经网络单元（RNN Cell）. 在每个时刻$t$，循环神经网络接受输入信息$\\boldsymbol{x}_t \\in \\mathbb{R}^{M}$，并与前一时刻的隐状态$\\boldsymbol{h}_{t-1} \\in \\mathbb{R}^D$一起进行计算，输出一个新的当前时刻的隐状态$\\boldsymbol{h}_t$.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{h}_t = f(\\boldsymbol{h}_{t-1}, \\boldsymbol{x}_t),\n",
    "$$\n",
    "\n",
    "其中$\\boldsymbol{h}_{0} = 0$，$f(\\cdot)$是一个非线性函数. \n",
    "\n",
    "循环神经网络的参数可以通过梯度下降法来学习。和前馈神经网络类似，我们可以使用随时间反向传播（BackPropagation Through Time，BPTT）算法高效地手工计算梯度，也可以使用自动微分的方法，通过计算图自动计算梯度。\n",
    "\n",
    "循环神经网络被认为是图灵完备的，一个完全连接的循环神经网络可以近似解决所有的可计算问题。然而，虽然理论上循环神经网络可以建立长时间间隔的状态之间的依赖关系，但是由于具体的实现方式和参数学习方式会导致梯度爆炸或梯度消失问题，实际上，通常循环神经网络只能学习到短期的依赖关系，很难建模这种长距离的依赖关系，称为长程依赖问题（Long-Term Dependencies Problem）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6.1 循环神经网络的记忆能力实验\n",
    "循环神经网络的一种简单实现是简单循环网络（Simple Recurrent Network，SRN）．\n",
    "\n",
    "令向量$\\boldsymbol{x}_t \\in \\mathbb{R}^M$表示在时刻$t$时网络的输入，$\\boldsymbol{h_t} \\in \\mathbb{R}^D$ 表示隐藏层状态（即隐藏层神经元活性值），则$\\boldsymbol{h}_t$不仅和当前时刻的输入$\\boldsymbol{x}_t$相关，也和上一个时刻的隐藏层状态$\\boldsymbol{h}_{t-1}$相关. 简单循环网络在时刻$t$的更新公式为\n",
    "\n",
    "$$\n",
    "\\boldsymbol{h}_t = f(\\boldsymbol{W}\\boldsymbol{x}_t + \\boldsymbol{U}\\boldsymbol{h}_{t-1} + b),\n",
    "$$\n",
    "\n",
    "其中$\\boldsymbol{h}_{t}$为隐状态向量，$\\boldsymbol{U} \\in \\mathbb{R}^{D\\times D}$为**状态-状态**权重矩阵，$\\boldsymbol{W} \\in \\mathbb{R}^{D\\times M}$为**状态-输入**权重矩阵，$\\boldsymbol{b}\\in \\mathbb{R}^{D}$为偏置向量。\n",
    "\n",
    "**图6.2** 展示了一个按时间展开的循环神经网络。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/a813c79080c84187ace2267f0c40352c61f69c8f5c7a4fa3a1f57eb24ed9fa27\" width=50%></center>\n",
    "<br><center>图6.2 循环神经网络结构</center></br>\n",
    "\n",
    "简单循环网络在参数学习时存在长程依赖问题，很难建模长时间间隔（Long Range）的状态之间的依赖关系。为了测试简单循环网络的记忆能力，本节构建一个数字求和任务进行实验。\n",
    "\n",
    "数字求和任务的输入是一串数字，前两个位置的数字为0-9，其余数字随机生成（主要为0），预测目标是输入序列中前两个数字的加和。图6.3展示了长度为10的数字序列．\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/40d2e25dfa5a44a386c6f98ff93e2c2a2a44bab0c1764c0ab22aec45c18e61f5\" width=50%></center>\n",
    "<br><center>图6.3 数字求和任务示例</center></br>\n",
    "\n",
    "\n",
    "如果序列长度越长，准确率越高，则说明网络的记忆能力越好．因此，我们可以构建不同长度的数据集，通过验证简单循环网络在不同长度的数据集上的表现，从而测试简单循环网络的长程依赖能力."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6.1.1 数据集构建\n",
    "\n",
    "我们首先构建不同长度的数字预测数据集DigitSum.\n",
    "\n",
    "#### 6.1.1.1 数据集的构建函数\n",
    "\n",
    "由于在本任务中，输入序列的前两位数字为 0 − 9，其组合数是固定的，所以可以穷举所有的前两位数字组合，并在后面默认用0填充到固定长度. 但考虑到数据的多样性，这里对生成的数字序列中的零位置进行随机采样，并将其随机替换成0-9的数字以增加样本的数量．\n",
    "\n",
    "我们可以通过设置$k$的数值来指定一条样本随机生成的数字序列数量.当生成某个指定长度的数据集时，会同时生成训练集、验证集和测试集。当$k$=3时，生成训练集。当$k$=1时，生成验证集和测试集. 代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 固定随机种子\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "def generate_data(length, k, save_path):\n",
    "    if length < 3:\n",
    "        raise ValueError(\"The length of data should be greater than 2.\")\n",
    "    if k == 0:\n",
    "        raise ValueError(\"k should be greater than 0.\")\n",
    "    # 生成100条长度为length的数字序列，除前两个字符外，序列其余数字暂用0填充\n",
    "    base_examples = []\n",
    "    for n1 in range(0, 10):\n",
    "        for n2 in range(0, 10):\n",
    "            seq = [n1, n2] + [0] * (length - 2)\n",
    "            label = n1 + n2\n",
    "            base_examples.append((seq, label))\n",
    "    \n",
    "    examples = []\n",
    "    # 数据增强：对base_examples中的每条数据，默认生成k条数据，放入examples\n",
    "    for base_example in base_examples:\n",
    "        for _ in range(k):\n",
    "            # 随机生成替换的元素位置和元素\n",
    "            idx = np.random.randint(2, length)\n",
    "            val = np.random.randint(0, 10)\n",
    "            # 对序列中的对应零元素进行替换\n",
    "            seq = base_example[0].copy()\n",
    "            label = base_example[1]\n",
    "            seq[idx] = val\n",
    "            examples.append((seq, label))\n",
    "\n",
    "    # 保存增强后的数据\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for example in examples:\n",
    "            # 将数据转为字符串类型，方便保存\n",
    "            seq = [str(e) for e in example[0]]\n",
    "            label = str(example[1])\n",
    "            line = \" \".join(seq) + \"\\t\" + label + \"\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "    print(f\"generate data to: {save_path}.\")\n",
    "\n",
    "# 定义生成的数字序列长度\n",
    "lengths = [5, 10, 15, 20, 25, 30, 35]\n",
    "for length in lengths:\n",
    "    # 生成长度为length的训练数据\n",
    "    save_path = f\"./datasets/{length}/train.txt\"\n",
    "    k = 3\n",
    "    generate_data(length, k, save_path)\n",
    "    # 生成长度为length的验证数据\n",
    "    save_path = f\"./datasets/{length}/dev.txt\"\n",
    "    k = 1\n",
    "    generate_data(length, k, save_path)\n",
    "    # 生成长度为length的测试数据\n",
    "    save_path = f\"./datasets/{length}/test.txt\"\n",
    "    k = 1\n",
    "    generate_data(length, k, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.1.1.2 加载数据并进行数据划分\n",
    "为方便使用，本实验提前生成了长度分别为5、10、 15、20、25、30和35的7份数据，存放于“./datasets”目录下，读者可以直接加载使用。代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# 加载数据\n",
    "def load_data(data_path):\n",
    "    # 加载训练集\n",
    "    train_examples = []\n",
    "    train_path = os.path.join(data_path, \"train.txt\")\n",
    "    with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            # 解析一行数据，将其处理为数字序列seq和标签label\n",
    "            items = line.strip().split(\"\\t\")\n",
    "            seq = [int(i) for i in items[0].split(\" \")]\n",
    "            label = int(items[1])\n",
    "            train_examples.append((seq, label))\n",
    "\n",
    "    # 加载验证集\n",
    "    dev_examples = []\n",
    "    dev_path = os.path.join(data_path, \"dev.txt\")\n",
    "    with open(dev_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            # 解析一行数据，将其处理为数字序列seq和标签label\n",
    "            items = line.strip().split(\"\\t\")\n",
    "            seq = [int(i) for i in items[0].split(\" \")]\n",
    "            label = int(items[1])\n",
    "            dev_examples.append((seq, label))\n",
    "\n",
    "    # 加载测试集\n",
    "    test_examples = []\n",
    "    test_path = os.path.join(data_path, \"test.txt\")\n",
    "    with open(test_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            # 解析一行数据，将其处理为数字序列seq和标签label\n",
    "            items = line.strip().split(\"\\t\")\n",
    "            seq = [int(i) for i in items[0].split(\" \")]\n",
    "            label = int(items[1])\n",
    "            test_examples.append((seq, label))\n",
    "\n",
    "    return train_examples, dev_examples, test_examples\n",
    "\n",
    "# 设定加载的数据集的长度\n",
    "length = 5\n",
    "# 该长度的数据集的存放目录\n",
    "data_path = f\"./datasets/{length}\"\n",
    "# 加载该数据集\n",
    "train_examples, dev_examples, test_examples = load_data(data_path)\n",
    "print(\"dev example:\", dev_examples[:2])\n",
    "print(\"训练集数量：\", len(train_examples))\n",
    "print(\"验证集数量：\", len(dev_examples))\n",
    "print(\"测试集数量：\", len(test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.1.1.3 构造Dataset类\n",
    "\n",
    "为了方便使用梯度下降法进行优化，我们构造了DigitSum数据集的Dataset类，函数`__getitem__`负责根据索引读取数据，并将数据转换为张量。代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddle.io import Dataset\n",
    "\n",
    "class DigitSumDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        seq = paddle.to_tensor(example[0], dtype=\"int64\")\n",
    "        label = paddle.to_tensor(example[1], dtype=\"int64\")\n",
    "        return seq, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6.1.2 模型构建\n",
    "\n",
    "使用SRN模型进行数字加和任务的模型结构为如图6.4所示.\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/4bdc5e83a4e24feba0c3300535454b5a2030e48a992b4c3184f86c032484a928\" width=50%></center>\n",
    "<br><center>图6.4 基于SRN模型的数字预测</center></br>\n",
    "\n",
    "整个模型由以下几个部分组成：  \n",
    "（1） 嵌入层：将输入的数字序列进行向量化，即将每个数字映射为向量；  \n",
    "（2） SRN 层：接收向量序列，更新循环单元，将最后时刻的隐状态作为整个序列的表示；  \n",
    "（3） 输出层：一个线性层，输出分类的结果.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.1.2.1 嵌入层\n",
    "\n",
    "本任务输入的样本是数字序列，为了更好地表示数字，需要将数字映射为一个嵌入（Embedding）向量。嵌入向量中的每个维度均能用来刻画该数字本身的某种特性。由于向量能够表达该数字更多的信息，利用向量进行数字求和任务，可以使得模型具有更强的拟合能力。\n",
    "\n",
    "首先，我们构建一个嵌入矩阵（Embedding Matrix）$\\boldsymbol{E}\\in \\mathbb{R}^{10\\times M}$，其中第$i$行对应数字$i$的嵌入向量，每个嵌入向量的维度是$M$。如图6.5所示。\n",
    "给定一个组数字序列$\\boldsymbol{S} \\in \\mathbb{R}^{B\\times L}$，其中$B$为批大小，$L$为序列长度，可以通过查表将其映射为嵌入表示$\\boldsymbol{X}\\in \\mathbb{R}^{B\\times L \\times M}$。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/22d9a00278914221a4074618567af6492323e5b2ed0c47849c8669540ece7dfb\" width=50%></center>\n",
    "<center>图6.5 嵌入矩阵</center>\n",
    "\n",
    "> **提醒**：为了和代码的实现保持一致性，这里使用形状为$(样本数量\\times 序列长度\\times 特征维度)$的张量来表示一组样本。\n",
    "\n",
    "\n",
    "\n",
    "或者也可以将每个数字表示为10维的one-hot向量，使用矩阵运算得到嵌入表示：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\boldsymbol{S}^{'} \\boldsymbol{E}，\n",
    "$$\n",
    "\n",
    "其中$\\boldsymbol{S}' \\in \\mathbb{R}^{B\\times L\\times 10}$是序列$\\boldsymbol{S}$对应的one-hot表示。\n",
    "\n",
    "> **思考**：如果不使用嵌入层，直接将数字作为SRN层输入有什么问题？\n",
    "\n",
    "\n",
    "基于索引方式的嵌入层的实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "class Embedding(nn.Layer):\n",
    "    def __init__(self, num_embeddings, embedding_dim, para_attr=paddle.ParamAttr(initializer=nn.initializer.XavierUniform())):\n",
    "        super(Embedding, self).__init__()\n",
    "        # 定义嵌入矩阵\n",
    "        self.W = paddle.create_parameter(shape=[num_embeddings, embedding_dim], dtype=\"float32\", attr=para_attr)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # 根据索引获取对应词向量\n",
    "        embs = self.W[inputs]\n",
    "        return embs\n",
    "\n",
    "emb_layer = Embedding(10, 5)\n",
    "inputs = paddle.to_tensor([0,1,2,3])\n",
    "emb_layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> **思考**：请同学们思考基于one-hot编码的嵌入层应该如何实现."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.1.2.2 SRN层\n",
    "\n",
    "数字序列$\\boldsymbol{S} \\in \\mathbb{R}^{B\\times L}$经过嵌入层映射后，转换为$\\boldsymbol{X}\\in \\mathbb{R}^{B\\times L\\times M}$，其中$B$为批大小，$L$为序列长度，$M$为嵌入维度。\n",
    "\n",
    "在时刻$t$，SRN将当前的输入$\\boldsymbol{X}_t \\in \\mathbb{R}^{B \\times M}$与隐状态$\\boldsymbol{H}_{t-1}  \\in \\mathbb{R}^{B \\times D}$进行线性变换和组合，并通过一个非线性激活函数$f(\\cdot)$得到新的隐状态，SRN的状态更新函数为:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{H}_t = \\text{Tanh}(\\boldsymbol{X}_t\\boldsymbol{W} + \\boldsymbol{H}_{t-1}\\boldsymbol{U} + \\boldsymbol{b}),\n",
    "$$\n",
    "\n",
    "其中$\\boldsymbol{W} \\in \\mathbb{R}^{M \\times D}, \\boldsymbol{U} \\in \\mathbb{R}^{D \\times D}, \\boldsymbol{b} \\in \\mathbb{R}^{1 \\times D}$是可学习参数，$D$表示隐状态向量的维度。\n",
    "\n",
    "简单循环网络的代码实现如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "paddle.seed(0)\n",
    "\n",
    "# SRN模型\n",
    "class SRN(nn.Layer):\n",
    "    def __init__(self, input_size,  hidden_size, W_attr=None, U_attr=None, b_attr=None):\n",
    "        super(SRN, self).__init__()\n",
    "        # 嵌入向量的维度\n",
    "        self.input_size = input_size\n",
    "        # 隐状态的维度\n",
    "        self.hidden_size = hidden_size\n",
    "        # 定义模型参数W，其shape为 input_size x hidden_size\n",
    "        self.W = paddle.create_parameter(shape=[input_size, hidden_size], dtype=\"float32\", attr=W_attr)\n",
    "        # 定义模型参数U，其shape为hidden_size x hidden_size\n",
    "        self.U = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\",attr=U_attr)\n",
    "        # 定义模型参数b，其shape为 1 x hidden_size\n",
    "        self.b = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\", attr=b_attr)\n",
    "\n",
    "    # 初始化向量\n",
    "    def init_state(self, batch_size):\n",
    "        hidden_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=\"float32\")\n",
    "        return hidden_state\n",
    "\n",
    "    # 定义前向计算\n",
    "    def forward(self, inputs, hidden_state=None):\n",
    "        # inputs: 输入数据, 其shape为batch_size x seq_len x input_size\n",
    "        batch_size, seq_len, input_size = inputs.shape\n",
    "\n",
    "        # 初始化起始状态的隐向量, 其shape为 batch_size x hidden_size\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self.init_state(batch_size)\n",
    "\n",
    "        # 循环执行RNN计算\n",
    "        for step in range(seq_len):\n",
    "            # 获取当前时刻的输入数据step_input, 其shape为 batch_size x input_size\n",
    "            step_input = inputs[:, step, :]\n",
    "            # 获取当前时刻的隐状态向量hidden_state, 其shape为 batch_size x hidden_size\n",
    "            hidden_state = F.tanh(paddle.matmul(step_input, self.W) + paddle.matmul(hidden_state, self.U) + self.b)\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> 提醒：  这里只保留了简单循环网络的最后一个时刻的输出向量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 初始化参数并运行\n",
    "\n",
    "W_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[0.1, 0.2], [0.1,0.2]]))\n",
    "U_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[0.0, 0.1], [0.1,0.0]]))\n",
    "b_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[0.1, 0.1]]))\n",
    "\n",
    "srn = SRN(2, 2, W_attr=W_attr, U_attr=U_attr, b_attr=b_attr)\n",
    "\n",
    "inputs = paddle.to_tensor([[[1, 0],[0, 2]]], dtype=\"float32\")\n",
    "hidden_state = srn(inputs)\n",
    "print(\"hidden_state\", hidden_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "飞桨框架已经内置了SRN的API `paddle.nn.SimpleRNN`，其与自己实现的SRN不同点在于其实现时采用了两个偏置，同时矩阵相乘时参数在输入数据前面，如下公式所示：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{H}_t = \\text{Tanh}(\\boldsymbol{W}\\boldsymbol{X}_t + \\boldsymbol{b}_x +  \\boldsymbol{U}\\boldsymbol{H}_{t-1} + \\boldsymbol{b}_h),\n",
    "$$\n",
    "\n",
    "其中$\\boldsymbol{W} \\in \\mathbb{R}^{M \\times D}, \\boldsymbol{U} \\in \\mathbb{R}^{D \\times D}, \\boldsymbol{b}_x \\in \\mathbb{R}^{1 \\times D}, \\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times D}$是可学习参数，$M$表示嵌入向量的维度，$D$表示隐状态向量的维度。\n",
    "\n",
    "另外，内置SRN API在执行完前向计算后，会返回两个参数：序列向量和最后时刻的隐状态向量。在飞桨实现时，考虑到了双向和多层SRN的因素，返回的向量附带了这些信息。\n",
    "\n",
    "其中序列向量outputs是指最后一层SRN的输出向量，其shape为[batch_size, seq_len, num_directions * hidden_size]；最后时刻的隐状态向量shape为[num_layers * num_directions, batch_size, hidden_size]。\n",
    "\n",
    "这里我们可以将自己实现的SRN和Paddle框架内置的SRN返回的结果进行打印展示，实现代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size\n",
    "batch_size, seq_len, input_size = 8, 20, 32\n",
    "inputs = paddle.randn(shape=[batch_size, seq_len, input_size])\n",
    "\n",
    "# 设置模型的hidden_size\n",
    "hidden_size = 32\n",
    "paddle_srn = nn.SimpleRNN(input_size, hidden_size)\n",
    "self_srn = SRN(input_size, hidden_size)\n",
    "\n",
    "self_hidden_state = self_srn(inputs)\n",
    "paddle_outputs, paddle_hidden_state = paddle_srn(inputs)\n",
    "\n",
    "print(\"self_srn hidden_state: \", self_hidden_state.shape)\n",
    "print(\"paddle_srn outpus:\", paddle_outputs.shape)\n",
    "print(\"paddle_srn hidden_state:\", paddle_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以看到，自己实现的SRN由于没有考虑多层因素，因此没有层次这个维度，因此其输出shape为[8, 32]。同时由于在以上代码使用Paddle内置API实例化SRN时，默认定义的是1层的单向SRN，因此其shape为[1, 8, 32]，同时隐状态向量为[8,20, 32].\n",
    "\n",
    "接下来，我们可以将自己实现的SRN与Paddle内置的SRN在输出值的精度上进行对比，这里首先根据Paddle内置的SRN实例化模型（为了进行对比，在实例化时只保留一个偏置，将偏置$b_x$设置为0），然后提取该模型对应的参数，使用该参数去初始化自己实现的SRN，从而保证两者在参数初始化时是一致的。\n",
    "\n",
    "在进行实验时，首先定义输入数据`inputs`，然后将该数据分别传入Paddle内置的SRN与自己实现的SRN模型中，最后通过对比两者的隐状态输出向量。代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paddle.seed(0)\n",
    "\n",
    "# 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size\n",
    "batch_size, seq_len, input_size, hidden_size = 2, 5, 10, 10\n",
    "inputs = paddle.randn(shape=[batch_size, seq_len, input_size])\n",
    "\n",
    "# 设置模型的hidden_size\n",
    "bx_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(paddle.zeros([hidden_size, ])))\n",
    "paddle_srn = nn.SimpleRNN(input_size, hidden_size, bias_ih_attr=bx_attr)\n",
    "\n",
    "# 获取paddle_srn中的参数，并设置相应的paramAttr,用于初始化SRN\n",
    "W_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(paddle_srn.weight_ih_l0.T))\n",
    "U_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(paddle_srn.weight_hh_l0.T))\n",
    "b_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(paddle_srn.bias_hh_l0))\n",
    "self_srn = SRN(input_size, hidden_size, W_attr=W_attr, U_attr=U_attr, b_attr=b_attr)\n",
    "\n",
    "# 进行前向计算，获取隐状态向量，并打印展示\n",
    "self_hidden_state = self_srn(inputs)\n",
    "paddle_outputs, paddle_hidden_state = paddle_srn(inputs)\n",
    "print(\"paddle SRN:\\n\", paddle_hidden_state.numpy().squeeze(0))\n",
    "print(\"self SRN:\\n\", self_hidden_state.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以看到，两者的输出基本是一致的。另外，还可以进行对比两者在运算速度方面的差异。代码实现如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size\n",
    "batch_size, seq_len, input_size, hidden_size = 2, 5, 10, 10\n",
    "inputs = paddle.randn(shape=[batch_size, seq_len, input_size])\n",
    "\n",
    "# 实例化模型\n",
    "self_srn = SRN(input_size, hidden_size)\n",
    "paddle_srn = nn.SimpleRNN(input_size, hidden_size)\n",
    "\n",
    "# 计算自己实现的SRN运算速度\n",
    "model_time = 0\n",
    "for i in range(100):\n",
    "    strat_time = time.time()\n",
    "    out = self_srn(inputs)\n",
    "    # 预热10次运算，不计入最终速度统计\n",
    "    if i < 10:\n",
    "        continue\n",
    "    end_time = time.time()\n",
    "    model_time += (end_time - strat_time)\n",
    "avg_model_time = model_time / 90\n",
    "print('self_srn speed:', avg_model_time, 's')\n",
    "\n",
    "# 计算Paddle内置的SRN运算速度\n",
    "model_time = 0\n",
    "for i in range(100):\n",
    "    strat_time = time.time()\n",
    "    out = paddle_srn(inputs)\n",
    "    # 预热10次运算，不计入最终速度统计\n",
    "    if i < 10:\n",
    "        continue\n",
    "    end_time = time.time()\n",
    "    model_time += (end_time - strat_time)\n",
    "avg_model_time = model_time / 90\n",
    "print('paddle_srn speed:', avg_model_time, 's')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以看到，由于Paddle内部相关算子由C++实现，Paddle框架实现的SRN的运行效率显著高于自己实现的SRN效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.1.2.3 线性层\n",
    "线性层会将最后一个时刻的隐状态向量$\\boldsymbol{H}_L \\in \\mathbb{R}^{B \\times D}$进行线性变换，输出分类的对数几率（Logits）为：\n",
    "$$\n",
    "\\boldsymbol{Y} = \\boldsymbol{H}_L \\boldsymbol{W}_o + \\boldsymbol{b}_o，\n",
    "$$\n",
    "\n",
    "其中$\\boldsymbol{W}_o \\in \\mathbb{R}^{D \\times 19}$，$\\boldsymbol{b}_o \\in \\mathbb{R}^{19}$为可学习的权重矩阵和偏置。\n",
    "\n",
    "> 提醒：在分类问题的实践中，我们通常只需要模型输出分类的对数几率（Logits），而不用输出每个类的概率。这需要损失函数可以直接接收对数几率来损失计算。\n",
    "\n",
    "线性层直接使用`paddle.nn.Linear`算子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.1.2.4 模型汇总\n",
    "\n",
    "\n",
    "在定义了每一层的算子之后，我们定义一个数字求和模型Model_RNN4SeqClass，该模型会将嵌入层、SRN层和线性层进行组合，以实现数字求和的功能.\n",
    "\n",
    "具体来讲，Model_RNN4SeqClass会接收一个SRN层实例，用于处理数字序列数据，同时在`__init__`函数中定义一个`Embedding`嵌入层，其会将输入的数字作为索引，输出对应的向量，最后会使用`paddle.nn.Linear`定义一个线性层。\n",
    "\n",
    "> 提醒：为了方便进行对比实验，我们将SRN层的实例化放在\\code{Model_RNN4SeqClass}类外面。通常情况下，模型内部算子的实例化是放在模型里面。\n",
    "\n",
    "在`forward`函数中，调用上文实现的嵌入层、SRN层和线性层处理数字序列，同时返回最后一个位置的隐状态向量。代码实现如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 基于RNN实现数字预测的模型\n",
    "class Model_RNN4SeqClass(nn.Layer):\n",
    "    def __init__(self, model, num_digits, input_size, hidden_size, num_classes):\n",
    "        super(Model_RNN4SeqClass, self).__init__()\n",
    "        # 传入实例化的RNN层，例如SRN\n",
    "        self.rnn_model = model\n",
    "        # 词典大小\n",
    "        self.num_digits = num_digits\n",
    "        # 嵌入向量的维度\n",
    "        self.input_size = input_size\n",
    "        # 定义Embedding层\n",
    "        self.embedding = Embedding(num_digits, input_size)\n",
    "        # 定义线性层\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 将数字序列映射为相应向量\n",
    "        inputs_emb = self.embedding(inputs)\n",
    "        # 调用RNN模型\n",
    "        hidden_state = self.rnn_model(inputs_emb)\n",
    "        # 使用最后一个时刻的状态进行数字预测\n",
    "        logits = self.linear(hidden_state)\n",
    "        return logits\n",
    "\n",
    "# 实例化一个input_size为4， hidden_size为5的SRN\n",
    "srn = SRN(4, 5)\n",
    "# 基于srn实例化一个数字预测模型实例\n",
    "model = Model_RNN4SeqClass(srn, 10, 4, 5, 19)\n",
    "# 生成一个shape为 2 x 3 的批次数据\n",
    "inputs = paddle.to_tensor([[1, 2, 3], [2, 3, 4]])\n",
    "# 进行模型前向预测\n",
    "logits = model(inputs)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6.1.3 模型训练\n",
    "\n",
    "#### 6.1.3.1 训练指定长度的数字预测模型\n",
    "\n",
    "基于RunnerV3类进行训练，只需要指定length便可以加载相应的数据。设置超参数，使用Adam优化器，学习率为 0.001，实例化模型，使用第4.5.4节定义的Accuracy计算准确率。使用Runner进行训练，训练回合数设为500。代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import paddle\n",
    "import numpy as np\n",
    "from nndl import Accuracy, RunnerV3\n",
    "\n",
    "# 训练轮次\n",
    "num_epochs = 500\n",
    "# 学习率\n",
    "lr = 0.001\n",
    "# 输入数字的类别数\n",
    "num_digits = 10\n",
    "# 将数字映射为向量的维度\n",
    "input_size = 32\n",
    "# 隐状态向量的维度\n",
    "hidden_size = 32\n",
    "# 预测数字的类别数\n",
    "num_classes = 19\n",
    "# 批大小 \n",
    "batch_size = 8\n",
    "# 模型保存目录\n",
    "save_dir = \"./checkpoints\"\n",
    "\n",
    "# 通过指定length进行不同长度数据的实验\n",
    "def train(length):\n",
    "    print(f\"\\n====> Training SRN with data of length {length}.\")\n",
    "    # 固定随机种子\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    paddle.seed(0)\n",
    "\n",
    "    # 加载长度为length的数据\n",
    "    data_path = f\"./datasets/{length}\"\n",
    "    train_examples, dev_examples, test_examples = load_data(data_path)\n",
    "    train_set, dev_set, test_set = DigitSumDataset(train_examples), DigitSumDataset(dev_examples), DigitSumDataset(test_examples)\n",
    "    train_loader = paddle.io.DataLoader(train_set, batch_size=batch_size)\n",
    "    dev_loader = paddle.io.DataLoader(dev_set, batch_size=batch_size)\n",
    "    test_loader = paddle.io.DataLoader(test_set, batch_size=batch_size)\n",
    "    # 实例化模型\n",
    "    base_model = SRN(input_size, hidden_size)\n",
    "    model = Model_RNN4SeqClass(base_model, num_digits, input_size, hidden_size, num_classes) \n",
    "    # 指定优化器\n",
    "    optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters()) \n",
    "    # 定义评价指标\n",
    "    metric = Accuracy()\n",
    "    # 定义损失函数\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 基于以上组件，实例化Runner\n",
    "    runner = RunnerV3(model, optimizer, loss_fn, metric)\n",
    "\n",
    "    # 进行模型训练\n",
    "    model_save_path = os.path.join(save_dir, f\"best_srn_model_{length}.pdparams\")\n",
    "    runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=100, log_steps=100, save_path=model_save_path)\n",
    "\n",
    "    return runner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.1.3.2 多组训练\n",
    "\n",
    "接下来，分别进行数据长度为10, 15, 20, 25, 30, 35的数字预测模型训练实验，训练后的`runner`保存至`runners`字典中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "srn_runners = {}\n",
    "\n",
    "lengths = [10, 15, 20, 25, 30, 35]\n",
    "for length in lengths:\n",
    "    runner = train(length)\n",
    "    srn_runners[length] = runner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.1.3.3 损失曲线展示\n",
    "定义`plot_training_loss`函数，分别画出各个长度的数字预测模型训练过程中，在训练集和验证集上的损失曲线，实现代码实现如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_loss(runner, fig_name, sample_step):\n",
    "\n",
    "    plt.figure()\n",
    "    train_items = runner.train_step_losses[::sample_step]\n",
    "    train_steps=[x[0] for x in train_items]\n",
    "    train_losses = [x[1] for x in train_items]\n",
    "    plt.plot(train_steps, train_losses, color='#e4007f', label=\"Train loss\")\n",
    "    \n",
    "    dev_steps=[x[0] for x in runner.dev_losses]\n",
    "    dev_losses = [x[1] for x in runner.dev_losses]\n",
    "    plt.plot(dev_steps, dev_losses, color='#f19ec2', linestyle='--', label=\"Dev loss\")\n",
    "\n",
    "    #绘制坐标轴和图例\n",
    "    plt.ylabel(\"loss\", fontsize='large')\n",
    "    plt.xlabel(\"step\", fontsize='large')\n",
    "    plt.legend(loc='upper right', fontsize='x-large')\n",
    "\n",
    "    plt.savefig(fig_name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 画出训练过程中的损失图\n",
    "for length in lengths:\n",
    "    runner = srn_runners[length]\n",
    "    fig_name = f\"./images/6.6_{length}.pdf\"\n",
    "    plot_training_loss(runner, fig_name, sample_step=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "图6.6展示了在6个数据集上的损失变化情况，数据集的长度分别为10、15、20、25、30和35. 从输出结果看，随着数据序列长度的增加，虽然训练集损失逐渐逼近于0，但是验证集损失整体趋向越来越大，这表明当序列变长时，SRN模型保持序列长期依赖能力在逐渐变弱，越来越无法学习到有用的知识.\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/7e6a091541664269b589b0b47906f7d38340d2f2506a49e0a514f9db85645fb4\" width=100%></center>\n",
    "<center>图6.6 SRN在不同长度数据集训练损失变化图</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6.1.4 模型评价\n",
    "在模型评价时，加载不同长度的效果最好的模型，然后使用测试集对该模型进行评价，观察模型在测试集上预测的准确度. 同时记录一下不同长度模型在训练过程中，在验证集上最好的效果。代码实现如下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "srn_dev_scores = []\n",
    "srn_test_scores = []\n",
    "for length in lengths:\n",
    "    print(f\"Evaluate SRN with data length {length}.\")\n",
    "    runner = srn_runners[length]\n",
    "    # 加载训练过程中效果最好的模型\n",
    "    model_path = os.path.join(save_dir, f\"best_srn_model_{length}.pdparams\")\n",
    "    runner.load_model(model_path)\n",
    "    \n",
    "    # 加载长度为length的数据\n",
    "    data_path = f\"./datasets/{length}\"\n",
    "    train_examples, dev_examples, test_examples = load_data(data_path)\n",
    "    test_set = DigitSumDataset(test_examples)\n",
    "    test_loader = paddle.io.DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "    # 使用测试集评价模型，获取测试集上的预测准确率\n",
    "    score, _ = runner.evaluate(test_loader)\n",
    "    srn_test_scores.append(score)\n",
    "    srn_dev_scores.append(max(runner.dev_scores))\n",
    "\n",
    "for length, dev_score, test_score in zip(lengths, srn_dev_scores, srn_test_scores):\n",
    "    print(f\"[SRN] length:{length}, dev_score: {dev_score}, test_score: {test_score: .5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接下来，将SRN在不同长度的验证集和测试集数据上的表现，绘制成图片进行观察。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lengths, srn_dev_scores, '-o', color='#e4007f',  label=\"Dev Accuracy\")\n",
    "plt.plot(lengths, srn_test_scores,'-o', color='#f19ec2', label=\"Test Accuracy\")\n",
    "\n",
    "#绘制坐标轴和图例\n",
    "plt.ylabel(\"accuracy\", fontsize='large')\n",
    "plt.xlabel(\"sequence length\", fontsize='large')\n",
    "plt.legend(loc='upper right', fontsize='x-large')\n",
    "\n",
    "fig_name = \"./images/6.7.pdf\"\n",
    "plt.savefig(fig_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "图6.7 展示了SRN模型在不同长度数据训练出来的最好模型在验证集和测试集上的表现。可以看到，随着序列长度的增加，验证集和测试集的准确度整体趋势是降低的，这同样说明SRN模型保持长期依赖的能力在不断降低.\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/8ba26ab23fff4a3f8000760a38dfbd2ca0a11abcec6e44a2a6eed22fcf5af894\" width=40%></center>\n",
    "<center>图6.7 SRN在不同长度的验证集和测试集的准确度变化图</center>\n",
    "\n",
    "> 动手练习 6.1  参考《神经网络与深度学习》中的公式(6.50)，改进SRN的循环单元，加入隐状态之间的残差连接，并重复数字求和实验。观察是否可以缓解长程依赖问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6.2 梯度爆炸实验\n",
    "\n",
    "造成简单循环网络较难建模长程依赖问题的原因有两个：梯度爆炸和梯度消失。一般来讲，循环网络的梯度爆炸问题比较容易解决，一般通过权重衰减或梯度截断可以较好地来避免；对于梯度消失问题，更加有效的方式是改变模型，比如通过长短期记忆网络LSTM来进行缓解。\n",
    "\n",
    "本节将首先进行复现简单循环网络中的梯度爆炸问题，然后尝试使用梯度截断的方式进行解决。这里采用长度为20的数据集进行实验，训练过程中将进行输出$W$,$U$,$b$的梯度向量的范数，以此来衡量梯度的变化情况。\n",
    "\n",
    "\n",
    "### 6.2.1 梯度打印函数\n",
    "使用`custom_print_log`实现了在训练过程中打印梯度的功能，`custom_print_log`需要接收runner的实例，并通过`model.named_parameters()`获取该模型中的参数名和参数值. 这里我们分别定义`W_list`, `U_list`和`b_list`，用于分别存储训练过程中参数$W, U 和 b$的梯度范数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_list = []\n",
    "U_list = []\n",
    "b_list = []\n",
    "# 计算梯度范数\n",
    "def custom_print_log(runner):\n",
    "    model = runner.model\n",
    "    W_grad_l2, U_grad_l2, b_grad_l2 = 0, 0, 0\n",
    "    for name, param in model.named_parameters(): \n",
    "        if name == \"rnn_model.W\":  \n",
    "            W_grad_l2 = paddle.norm(param.grad, p=2).numpy()[0]\n",
    "        if name == \"rnn_model.U\": \n",
    "            U_grad_l2 = paddle.norm(param.grad, p=2).numpy()[0]\n",
    "        if name == \"rnn_model.b\": \n",
    "            b_grad_l2 = paddle.norm(param.grad, p=2).numpy()[0]\n",
    "    print(f\"[Training] W_grad_l2: {W_grad_l2:.5f}, U_grad_l2: {U_grad_l2:.5f}, b_grad_l2: {b_grad_l2:.5f} \") \n",
    "    W_list.append(W_grad_l2)\n",
    "    U_list.append(U_grad_l2)\n",
    "    b_list.append(b_grad_l2)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6.2.2 复现梯度爆炸现象\n",
    "为了更好地复现梯度爆炸问题，使用SGD优化器将批大小和学习率调大，学习率为0.2，同时在计算交叉熵损失时，将reduction设置为sum，表示将损失进行累加。 代码实现如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import paddle\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "paddle.seed(0)\n",
    "\n",
    "# 训练轮次\n",
    "num_epochs = 50\n",
    "# 学习率\n",
    "lr = 0.2\n",
    "# 输入数字的类别数\n",
    "num_digits = 10\n",
    "# 将数字映射为向量的维度\n",
    "input_size = 32\n",
    "# 隐状态向量的维度\n",
    "hidden_size = 32\n",
    "# 预测数字的类别数\n",
    "num_classes = 19\n",
    "# 批大小 \n",
    "batch_size = 64\n",
    "# 模型保存目录\n",
    "save_dir = \"./checkpoints\"\n",
    "\n",
    "\n",
    "# 可以设置不同的length进行不同长度数据的预测实验\n",
    "length = 20\n",
    "print(f\"\\n====> Training SRN with data of length {length}.\")\n",
    "\n",
    "# 加载长度为length的数据\n",
    "data_path = f\"./datasets/{length}\"\n",
    "train_examples, dev_examples, test_examples = load_data(data_path)\n",
    "train_set, dev_set, test_set = DigitSumDataset(train_examples), DigitSumDataset(dev_examples),DigitSumDataset(test_examples)\n",
    "train_loader = paddle.io.DataLoader(train_set, batch_size=batch_size)\n",
    "dev_loader = paddle.io.DataLoader(dev_set, batch_size=batch_size)\n",
    "test_loader = paddle.io.DataLoader(test_set, batch_size=batch_size)\n",
    "# 实例化模型\n",
    "base_model = SRN(input_size, hidden_size)\n",
    "model = Model_RNN4SeqClass(base_model, num_digits, input_size, hidden_size, num_classes) \n",
    "# 指定优化器\n",
    "optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters=model.parameters()) \n",
    "# 定义评价指标\n",
    "metric = Accuracy()\n",
    "# 定义损失函数\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "# 基于以上组件，实例化Runner\n",
    "runner = RunnerV3(model, optimizer, loss_fn, metric)\n",
    "\n",
    "# 进行模型训练\n",
    "model_save_path = os.path.join(save_dir, f\"srn_explosion_model_{length}.pdparams\")\n",
    "runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=100, log_steps=1, \n",
    "             save_path=model_save_path, custom_print_log=custom_print_log)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接下来，可以获取训练过程中关于$\\boldsymbol{W}$，$\\boldsymbol{U}$和$\\boldsymbol{b}$参数梯度的L2范数，并将其绘制为图片以便展示，相应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_grad(W_list, U_list, b_list, save_path, keep_steps=40):\n",
    "\n",
    "    # 开始绘制图片\n",
    "    plt.figure()\n",
    "    # 默认保留前40步的结果\n",
    "    steps = list(range(keep_steps))\n",
    "    plt.plot(steps, W_list[:keep_steps], \"r-\", color=\"#e4007f\", label=\"W_grad_l2\")\n",
    "    plt.plot(steps, U_list[:keep_steps], \"-.\", color=\"#f19ec2\", label=\"U_grad_l2\")\n",
    "    plt.plot(steps, b_list[:keep_steps], \"--\", color=\"#000000\", label=\"b_grad_l2\")\n",
    "    \n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(\"L2 Norm\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.savefig(save_path)\n",
    "    print(\"image has been saved to: \", save_path)\n",
    "\n",
    "save_path =  f\"./images/6.8.pdf\"\n",
    "plot_grad(W_list, U_list, b_list, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "图6.8 展示了在训练过程中关于$\\boldsymbol{W}$，$\\boldsymbol{U}$和$\\boldsymbol{b}$参数梯度的L2范数，可以看到经过学习率等方式的调整，梯度范数急剧变大，而后梯度范数几乎为0. 这是因为$\\text{Tanh}$为$\\text{Sigmoid}$型函数，其饱和区的导数接近于0，由于梯度的急剧变化，参数数值变的较大或较小，容易落入梯度饱和区，导致梯度为0，模型很难继续训练.\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/8942af416fdc461cb0115956598b32a4d83f147aee0d4ec8a9ebff43e65e85bf\" width=50%></center>\n",
    "<center>图6.8 梯度变化图</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接下来，使用该模型在测试集上进行测试。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"Evaluate SRN with data length {length}.\")\n",
    "# 加载训练过程中效果最好的模型\n",
    "model_path = os.path.join(save_dir, f\"srn_explosion_model_{length}.pdparams\")\n",
    "runner.load_model(model_path)\n",
    "\n",
    "# 使用测试集评价模型，获取测试集上的预测准确率\n",
    "score, _ = runner.evaluate(test_loader)\n",
    "print(f\"[SRN] length:{length}, Score: {score: .5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6.2.3 使用梯度截断解决梯度爆炸问题\n",
    "\n",
    "梯度截断是一种可以有效解决梯度爆炸问题的启发式方法，当梯度的模大于一定阈值时，就将它截断成为一个较小的数。一般有两种截断方式：按值截断和按模截断．本实验使用按模截断的方式解决梯度爆炸问题。\n",
    "\n",
    "按模截断是按照梯度向量$\\boldsymbol{g}$的模进行截断，保证梯度向量的模值不大于阈值$b$，裁剪后的梯度为:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{g} = \\left\\{\\begin{matrix} \\boldsymbol{g},  &  ||\\boldsymbol{g}||\\leq b \\\\ \\frac{b}{||\\boldsymbol{g}||} * \\boldsymbol{g},   &  ||\\boldsymbol{g}||\\gt b \\end{matrix} \\right..\n",
    "$$\n",
    "\n",
    "当梯度向量$\\boldsymbol{g}$的模不大于阈值$b$时，$\\boldsymbol{g}$数值不变，否则对$\\boldsymbol{g}$进行数值缩放。\n",
    "\n",
    "\n",
    "> 在飞桨中，可以使用[paddle.nn.ClipGradByNorm](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/ClipGradByNorm_cn.html#clipgradbynorm)进行按模截断. 在代码实现时，将ClipGradByNorm传入优化器，优化器在反向迭代过程中，每次梯度更新时默认可以对所有梯度裁剪。\n",
    "\n",
    "在引入梯度截断之后，将重新观察模型的训练情况。这里我们重新实例化一下：模型和优化器，然后组装runner，进行训练。代码实现如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 清空梯度列表\n",
    "W_list.clear()\n",
    "U_list.clear()\n",
    "b_list.clear()\n",
    "# 实例化模型\n",
    "base_model = SRN(input_size, hidden_size)\n",
    "model = Model_RNN4SeqClass(base_model, num_digits, input_size, hidden_size, num_classes) \n",
    "\n",
    "# 定义clip，并实例化优化器\n",
    "clip = nn.ClipGradByNorm(clip_norm=5.0)\n",
    "optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters=model.parameters(), grad_clip=clip)\n",
    "# 定义评价指标\n",
    "metric = Accuracy()\n",
    "# 定义损失函数\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "# 实例化Runner\n",
    "runner = RunnerV3(model, optimizer, loss_fn, metric)\n",
    "\n",
    "# 训练模型\n",
    "model_save_path = os.path.join(save_dir, f\"srn_fix_explosion_model_{length}.pdparams\")\n",
    "runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=100, log_steps=1, save_path=model_save_path, custom_print_log=custom_print_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在引入梯度截断后，获取训练过程中关于$\\boldsymbol{W}$，$\\boldsymbol{U}$和$\\boldsymbol{b}$参数梯度的L2范数，并将其绘制为图片以便展示，相应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_path =  f\"./images/6.9.pdf\"\n",
    "plot_grad(W_list, U_list, b_list, save_path, keep_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**图6.9** 展示了引入按模截断的策略之后，模型训练时参数梯度的变化情况。可以看到，随着迭代步骤的进行，梯度始终保持在一个有值的状态，表明按模截断能够很好地解决梯度爆炸的问题.\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/c80fb991231c417dad312c6c1396fdbd0db944566c2c46f29c4772387bcee278\" width=40%></center>\n",
    "<br><center>图6.9 增加梯度截断策略后，SRN参数梯度L2范数变化趋势</center></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接下来，使用梯度截断策略的模型在测试集上进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"Evaluate SRN with data length {length}.\")\n",
    "\n",
    "# 加载训练过程中效果最好的模型\n",
    "model_path = os.path.join(save_dir, f\"srn_fix_explosion_model_{length}.pdparams\")\n",
    "runner.load_model(model_path)\n",
    "\n",
    "# 使用测试集评价模型，获取测试集上的预测准确率\n",
    "score, _ = runner.evaluate(test_loader)\n",
    "print(f\"[SRN] length:{length}, Score: {score: .5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "由于为复现梯度爆炸现象，改变了学习率，优化器等，因此准确率相对比较低。但由于采用梯度截断策略后，在后续训练过程中，模型参数能够被更新优化，因此准确率有一定的提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6.3 LSTM的记忆能力实验\n",
    "\n",
    "长短期记忆网络（Long Short-Term Memory Network，LSTM）是一种可以有效缓解长程依赖问题的循环神经网络．LSTM 的特点是引入了一个新的内部状态（Internal State）$c \\in \\mathbb{R}^D$ 和门控机制（Gating Mechanism）．不同时刻的内部状态以近似线性的方式进行传递，从而缓解梯度消失或梯度爆炸问题．同时门控机制进行信息筛选，可以有效地增加记忆能力．例如，输入门可以让网络忽略无关紧要的输入信息，遗忘门可以使得网络保留有用的历史信息．在上一节的数字求和任务中，如果模型能够记住前两个非零数字，同时忽略掉一些不重要的干扰信息，那么即时序列很长，模型也有效地进行预测.\n",
    "\n",
    "LSTM 模型在第 $t$ 步时，循环单元的内部结构如图6.10所示．\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/6ddebc52f6494af49d1c77c1548ec4086c9a466869fd4389b3a02df3333e271a\" width=700></center>\n",
    "<br><center>图6.10 LSTM网络的循环单元结构</center></br>\n",
    "\n",
    "> **提醒**：为了和代码的实现保存一致性，这里使用形状为 (样本数量 × 序列长度 × 特征维度) 的张量来表示一组样本.\n",
    "\n",
    "假设一组输入序列为$\\boldsymbol{X}\\in \\mathbb{R}^{B\\times L\\times M}$，其中$B$为批大小，$L$为序列长度，$M$为输入特征维度，LSTM从从左到右依次扫描序列，并通过循环单元计算更新每一时刻的状态内部状态$\\boldsymbol{C}_{t}  \\in \\mathbb{R}^{B \\times D}$和输出状态$\\boldsymbol{H}_{t}  \\in \\mathbb{R}^{B \\times D}$。\n",
    "\n",
    "具体计算分为三步：\n",
    "\n",
    "**（1）计算三个“门”**\n",
    "\n",
    "在时刻$t$，LSTM的循环单元将当前时刻的输入$\\boldsymbol{X}_t \\in \\mathbb{R}^{B \\times M}$与上一时刻的输出状态$\\boldsymbol{H}_{t-1}  \\in \\mathbb{R}^{B \\times D}$，计算一组输入门$\\boldsymbol{I}_t$、遗忘门$\\boldsymbol{F}_t$和输出门$\\boldsymbol{O}_t$，其计算公式为\n",
    "\n",
    "$$\n",
    "\\boldsymbol{I}_{t}=\\sigma(\\boldsymbol{X}_t\\boldsymbol{W}_i+\\boldsymbol{H}_{t-1}\\boldsymbol{U}_i+\\boldsymbol{b}_i) \\in \\mathbb{R}^{B \\times D},\\\\\n",
    "\\boldsymbol{F}_{t}=\\sigma(\\boldsymbol{X}_t\\boldsymbol{W}_f+\\boldsymbol{H}_{t-1}\\boldsymbol{U}_f+\\boldsymbol{b}_f) \\in \\mathbb{R}^{B \\times D},\\\\\n",
    "\\boldsymbol{O}_{t}=\\sigma(\\boldsymbol{X}_t\\boldsymbol{W}_o+\\boldsymbol{H}_{t-1}\\boldsymbol{U}_o+\\boldsymbol{b}_o) \\in \\mathbb{R}^{B \\times D},\n",
    "$$\n",
    "\n",
    "其中$\\boldsymbol{W}_* \\in \\mathbb{R}^{M \\times D},\\boldsymbol{U}_* \\in \\mathbb{R}^{D \\times D},\\boldsymbol{b}_* \\in \\mathbb{R}^{D}$为可学习的参数，$\\sigma$表示Logistic函数，将“门”的取值控制在$(0,1)$区间。这里的“门”都是$B$个样本组成的矩阵，每一行为一个样本的“门”向量。\n",
    "\n",
    "**（2）计算内部状态**\n",
    "\n",
    "首先计算候选内部状态：\n",
    "\n",
    "$$\n",
    "\\tilde{\\boldsymbol{C}}_{t}=\\tanh(\\boldsymbol{X}_t\\boldsymbol{W}_c+\\boldsymbol{H}_{t-1}\\boldsymbol{U}_c+\\boldsymbol{b}_c) \\in \\mathbb{R}^{B \\times D},\n",
    "$$\n",
    "\n",
    "其中$\\boldsymbol{W}_c \\in \\mathbb{R}^{M \\times D}, \\boldsymbol{U}_c \\in \\mathbb{R}^{D \\times D},\\boldsymbol{b}_c \\in \\mathbb{R}^{D}$为可学习的参数。\n",
    "\n",
    "使用遗忘门和输入门，计算时刻$t$的内部状态：\n",
    "$$\n",
    "\\boldsymbol{C}_{t} = \\boldsymbol{F}_t \\odot \\boldsymbol{C}_{t-1} + \\boldsymbol{I}_{t} \\odot \\boldsymbol{\\tilde{C}}_{t},\n",
    "$$\n",
    "其中$\\odot$为逐元素积。\n",
    "\n",
    "**3）计算输出状态**\n",
    "当前LSTM单元状态（候选状态）的计算公式为:\n",
    "LSTM单元状态向量$\\boldsymbol{C}_{t}$和$\\boldsymbol{H}_t$的计算公式为\n",
    "$$\n",
    "\\boldsymbol{C}_{t} = \\boldsymbol F_t \\odot \\boldsymbol{C}_{t-1} + \\boldsymbol{I}_{t} \\odot \\boldsymbol{\\tilde{C}}_{t}，\\\\\n",
    "\\boldsymbol{H}_{t} = \\boldsymbol{O}_{t} \\odot \\text{tanh}(\\boldsymbol{C}_{t}).\n",
    "$$\n",
    "\n",
    "LSTM循环单元结构的输入是$t-1$时刻内部状态向量$\\boldsymbol{C}_{t-1} \\in \\mathbb{R}^{B \\times D}$和隐状态向量$\\boldsymbol{H}_{t-1} \\in \\mathbb{R}^{B \\times D}$，输出是当前时刻$t$的状态向量$\\boldsymbol{C}_{t} \\in \\mathbb{R}^{B \\times D}$和隐状态向量$\\boldsymbol{H}_{t} \\in \\mathbb{R}^{B \\times D}$。通过LSTM循环单元，整个网络可以建立较长距离的时序依赖关系。\n",
    "\n",
    "\n",
    "通过学习这些门的设置，LSTM可以选择性地忽略或者强化当前的记忆或是输入信息，帮助网络更好地学习长句子的语义信息。\n",
    "\n",
    "在本节中，我们使用LSTM模型重新进行数字求和实验，验证LSTM模型的长程依赖能力。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6.3.1 模型构建\n",
    "在本实验中，我们将使用第6.1.2.4节中定义Model_RNN4SeqClass模型，并构建 LSTM 算子．只需要实例化 LSTM 算，并传入Model_RNN4SeqClass模型，就可以用 LSTM 进行数字求和实验\n",
    "\n",
    "#### 6.3.1.1 LSTM层\n",
    "\n",
    "LSTM层的代码与SRN层结构相似，只是在SRN层的基础上增加了内部状态、输入门、遗忘门和输出门的定义和计算。这里LSTM层的输出也依然为序列的最后一个位置的隐状态向量。代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn.functional as F\n",
    "# 声明LSTM和相关参数\n",
    "class LSTM(nn.Layer):\n",
    "    def __init__(self, input_size, hidden_size, Wi_attr=None, Wf_attr=None, Wo_attr=None, Wc_attr=None,\n",
    "                 Ui_attr=None, Uf_attr=None, Uo_attr=None, Uc_attr=None, bi_attr=None, bf_attr=None,\n",
    "                 bo_attr=None, bc_attr=None):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # 初始化模型参数\n",
    "        self.W_i = paddle.create_parameter(shape=[input_size, hidden_size], dtype=\"float32\", attr=Wi_attr)\n",
    "        self.W_f = paddle.create_parameter(shape=[input_size, hidden_size], dtype=\"float32\", attr=Wf_attr)\n",
    "        self.W_o = paddle.create_parameter(shape=[input_size, hidden_size], dtype=\"float32\", attr=Wo_attr)\n",
    "        self.W_c = paddle.create_parameter(shape=[input_size, hidden_size], dtype=\"float32\", attr=Wc_attr)\n",
    "        self.U_i = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\", attr=Ui_attr)\n",
    "        self.U_f = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\", attr=Uf_attr)\n",
    "        self.U_o = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\", attr=Uo_attr)\n",
    "        self.U_c = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\", attr=Uc_attr)\n",
    "        self.b_i = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\", attr=bi_attr)\n",
    "        self.b_f = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\", attr=bf_attr)\n",
    "        self.b_o = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\", attr=bo_attr)\n",
    "        self.b_c = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\", attr=bc_attr)\n",
    "    \n",
    "    # 初始化状态向量和隐状态向量\n",
    "    def init_state(self, batch_size):\n",
    "        hidden_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=\"float32\")\n",
    "        cell_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=\"float32\")\n",
    "        return hidden_state, cell_state\n",
    "\n",
    "    # 定义前向计算\n",
    "    def forward(self, inputs, states=None):\n",
    "        # inputs: 输入数据，其shape为batch_size x seq_len x input_size\n",
    "        batch_size, seq_len, input_size = inputs.shape \n",
    "        \n",
    "        # 初始化起始的单元状态和隐状态向量，其shape为batch_size x hidden_size\n",
    "        if states is None:\n",
    "            states = self.init_state(batch_size)\n",
    "        hidden_state, cell_state = states\n",
    "\n",
    "        # 执行LSTM计算，包括：输入门、遗忘门和输出门、候选内部状态、内部状态和隐状态向量\n",
    "        for step in range(seq_len):\n",
    "            # 获取当前时刻的输入数据step_input: 其shape为batch_size x input_size\n",
    "            step_input = inputs[:, step, :]\n",
    "            # 计算输入门, 遗忘门和输出门, 其shape为：batch_size x hidden_size\n",
    "            I_gate = F.sigmoid(paddle.matmul(step_input, self.W_i) + paddle.matmul(hidden_state, self.U_i) + self.b_i)\n",
    "            F_gate = F.sigmoid(paddle.matmul(step_input, self.W_f) + paddle.matmul(hidden_state, self.U_f) + self.b_f)\n",
    "            O_gate = F.sigmoid(paddle.matmul(step_input, self.W_o) + paddle.matmul(hidden_state, self.U_o) + self.b_o)\n",
    "            # 计算候选状态向量, 其shape为：batch_size x hidden_size\n",
    "            C_tilde = F.tanh(paddle.matmul(step_input, self.W_c) + paddle.matmul(hidden_state, self.U_c) + self.b_c)\n",
    "            # 计算单元状态向量, 其shape为：batch_size x hidden_size\n",
    "            cell_state = F_gate * cell_state + I_gate * C_tilde\n",
    "            # 计算隐状态向量，其shape为：batch_size x hidden_size\n",
    "            hidden_state = O_gate * F.tanh(cell_state)\n",
    "\n",
    "        return hidden_state\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Wi_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[0.1, 0.2], [0.1, 0.2]]))\n",
    "Wf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[0.1, 0.2], [0.1, 0.2]]))\n",
    "Wo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[0.1, 0.2], [0.1, 0.2]]))\n",
    "Wc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[0.1, 0.2], [0.1, 0.2]]))\n",
    "Ui_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[0.0, 0.1], [0.1, 0.0]]))\n",
    "Uf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[0.0, 0.1], [0.1, 0.0]]))\n",
    "Uo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[0.0, 0.1], [0.1, 0.0]]))\n",
    "Uc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[0.0, 0.1], [0.1, 0.0]]))\n",
    "bi_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[0.1, 0.1]]))\n",
    "bf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[0.1, 0.1]]))\n",
    "bo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[0.1, 0.1]]))\n",
    "bc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[0.1, 0.1]]))\n",
    "\n",
    "lstm = LSTM(2, 2, Wi_attr=Wi_attr, Wf_attr=Wf_attr, Wo_attr=Wo_attr, Wc_attr=Wc_attr,\n",
    "                 Ui_attr=Ui_attr, Uf_attr=Uf_attr, Uo_attr=Uo_attr, Uc_attr=Uc_attr,\n",
    "                 bi_attr=bi_attr, bf_attr=bf_attr, bo_attr=bo_attr, bc_attr=bc_attr)\n",
    "\n",
    "inputs = paddle.to_tensor([[[1, 0]]], dtype=\"float32\")\n",
    "hidden_state = lstm(inputs)\n",
    "print(hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "飞桨框架已经内置了LSTM的API `paddle.nn.LSTM`，其与自己实现的SRN不同点在于其实现时采用了两个偏置，同时矩阵相乘时参数在输入数据前面，如下公式所示：。\n",
    "\n",
    "$$\n",
    "\\boldsymbol{I}_{t}=\\sigma(\\boldsymbol{W}_{ii}\\boldsymbol{X}_t + \\boldsymbol{b}_{ii} + \\boldsymbol{U}_{hi}\\boldsymbol{H}_{t-1}+\\boldsymbol{b}_{hi}) \\\\\n",
    "\\boldsymbol{F}_{t}=\\sigma(\\boldsymbol{W}_{if}\\boldsymbol{X}_t + \\boldsymbol{b}_{if}+ \\boldsymbol{U}_{hf}\\boldsymbol{H}_{t-1}+\\boldsymbol{b}_{hf}) \\\\\n",
    "\\boldsymbol{O}_{t}=\\sigma(\\boldsymbol{W}_{io}\\boldsymbol{X}_t+ \\boldsymbol{b}_{io} +\\boldsymbol{U}_{ho}\\boldsymbol{H}_{t-1}+\\boldsymbol{b}_{ho}),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{\\boldsymbol{C}}_{t}=\\tanh(\\boldsymbol{W}_{ic}\\boldsymbol{X}_t+\\boldsymbol{b}_{ic}+\\boldsymbol{U}_{hc}\\boldsymbol{H}_{t-1}+\\boldsymbol{b}_{hc}) ,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{C}_{t} = \\boldsymbol F_t \\cdot \\boldsymbol{C}_{t-1} + \\boldsymbol{I}_{t} \\cdot \\boldsymbol{\\tilde{C}}_{t}，\\\\\n",
    "\\boldsymbol{H}_{t} = \\boldsymbol{O}_{t} \\cdot \\text{tanh}(\\boldsymbol{C}_{t}).\n",
    "$$\n",
    "\n",
    "其中$\\boldsymbol{W}_* \\in \\mathbb{R}^{M \\times D}, \\boldsymbol{U}_* \\in \\mathbb{R}^{D \\times D}, \\boldsymbol{b}_{i*} \\in \\mathbb{R}^{1 \\times D}, \\boldsymbol{b}_{h*} \\in \\mathbb{R}^{1 \\times D}$是可学习参数。\n",
    "\n",
    "另外，在Paddle内置LSTM实现时，对于参数$\\boldsymbol{W}_{ii}, \\boldsymbol{W}_{if}, \\boldsymbol{W}_{io}, \\boldsymbol{W}_{ic}$ ，并不是分别申请这些矩阵，而是申请了一个大的矩阵$\\boldsymbol{W}_{ih}$，将这个大的矩阵分割为4份，便可以得到$\\boldsymbol{W}_{ii}, \\boldsymbol{W}_{if},\\boldsymbol{W}_{ic},\\boldsymbol{W}_{io}$。 同理，将会得到$\\boldsymbol{W}_{hh}$, $\\boldsymbol{b}_{ih}$和$\\boldsymbol{b}_{hh}$.\n",
    "\n",
    "最后，Paddle内置LSTM API将会返回参数序列向量outputs和最后时刻的状态向量，其中序列向量outputs是指最后一层SRN的输出向量，其shape为[batch_size, seq_len, num_directions * hidden_size]；最后时刻的状态向量是个元组，其包含了两个向量，分别是隐状态向量和单元状态向量，其shape均为[num_layers * num_directions, batch_size, hidden_size]。\n",
    "\n",
    "这里我们可以将自己实现的SRN和Paddle框架内置的SRN返回的结果进行打印展示，实现代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size\n",
    "batch_size, seq_len, input_size = 8, 20, 32\n",
    "inputs = paddle.randn(shape=[batch_size, seq_len, input_size])\n",
    "\n",
    "# 设置模型的hidden_size\n",
    "hidden_size = 32\n",
    "paddle_lstm = nn.LSTM(input_size, hidden_size)\n",
    "self_lstm = LSTM(input_size, hidden_size)\n",
    "\n",
    "self_hidden_state = self_lstm(inputs)\n",
    "paddle_outputs, (paddle_hidden_state, paddle_cell_state) = paddle_lstm(inputs)\n",
    "\n",
    "print(\"self_lstm hidden_state: \", self_hidden_state.shape)\n",
    "print(\"paddle_lstm outpus:\", paddle_outputs.shape)\n",
    "print(\"paddle_lstm hidden_state:\", paddle_hidden_state.shape)\n",
    "print(\"paddle_lstm cell_state:\", paddle_cell_state.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "可以看到，自己实现的LSTM由于没有考虑多层因素，因此没有层次这个维度，因此其输出shape为[8, 32]。同时由于在以上代码使用Paddle内置API实例化LSTM时，默认定义的是1层的单向SRN，因此其shape为[1, 8, 32]，同时隐状态向量为[8,20, 32].\n",
    "\n",
    "接下来，我们可以将自己实现的LSTM与Paddle内置的LSTM在输出值的精度上进行对比，这里首先根据Paddle内置的LSTM实例化模型（为了进行对比，在实例化时只保留一个偏置，将偏置$b_{ih}$设置为0），然后提取该模型对应的参数，进行参数分割后，使用相应参数去初始化自己实现的LSTM，从而保证两者在参数初始化时是一致的。\n",
    "\n",
    "\n",
    "\n",
    "在进行实验时，首先定义输入数据`inputs`，然后将该数据分别传入Paddle内置的LSTM与自己实现的LSTM模型中，最后通过对比两者的隐状态输出向量。代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "paddle.seed(0)\n",
    "\n",
    "# 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size\n",
    "batch_size, seq_len, input_size, hidden_size = 2, 5, 10, 10\n",
    "inputs = paddle.randn(shape=[batch_size, seq_len, input_size])\n",
    "\n",
    "# 设置模型的hidden_size\n",
    "bih_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(paddle.zeros([4*hidden_size, ])))\n",
    "paddle_lstm = nn.LSTM(input_size, hidden_size, bias_ih_attr=bih_attr)\n",
    "\n",
    "# 获取paddle_lstm中的参数，并设置相应的paramAttr,用于初始化lstm\n",
    "print(paddle_lstm.weight_ih_l0.T.shape)\n",
    "chunked_W = paddle.split(paddle_lstm.weight_ih_l0.T, num_or_sections=4, axis=-1)\n",
    "chunked_U = paddle.split(paddle_lstm.weight_hh_l0.T, num_or_sections=4, axis=-1)\n",
    "chunked_b = paddle.split(paddle_lstm.bias_hh_l0.T, num_or_sections=4, axis=-1)\n",
    "\n",
    "Wi_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_W[0]))\n",
    "Wf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_W[1]))\n",
    "Wc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_W[2]))\n",
    "Wo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_W[3]))\n",
    "Ui_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_U[0]))\n",
    "Uf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_U[1]))\n",
    "Uc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_U[2]))\n",
    "Uo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_U[3]))\n",
    "bi_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_b[0]))\n",
    "bf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_b[1]))\n",
    "bc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_b[2]))\n",
    "bo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_b[3]))\n",
    "self_lstm = LSTM(input_size, hidden_size, Wi_attr=Wi_attr, Wf_attr=Wf_attr, Wo_attr=Wo_attr, Wc_attr=Wc_attr,\n",
    "                 Ui_attr=Ui_attr, Uf_attr=Uf_attr, Uo_attr=Uo_attr, Uc_attr=Uc_attr,\n",
    "                 bi_attr=bi_attr, bf_attr=bf_attr, bo_attr=bo_attr, bc_attr=bc_attr)\n",
    "\n",
    "# 进行前向计算，获取隐状态向量，并打印展示\n",
    "self_hidden_state = self_lstm(inputs)\n",
    "paddle_outputs, (paddle_hidden_state, _) = paddle_lstm(inputs)\n",
    "print(\"paddle SRN:\\n\", paddle_hidden_state.numpy().squeeze(0))\n",
    "print(\"self SRN:\\n\", self_hidden_state.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以看到，两者的输出基本是一致的。另外，还可以进行对比两者在运算速度方面的差异。代码实现如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size\n",
    "batch_size, seq_len, input_size = 8, 20, 32\n",
    "inputs = paddle.randn(shape=[batch_size, seq_len, input_size])\n",
    "\n",
    "# 设置模型的hidden_size\n",
    "hidden_size = 32\n",
    "self_lstm = LSTM(input_size, hidden_size)\n",
    "paddle_lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "# 计算自己实现的SRN运算速度\n",
    "model_time = 0\n",
    "for i in range(100):\n",
    "    strat_time = time.time()\n",
    "    hidden_state = self_lstm(inputs)\n",
    "    # 预热10次运算，不计入最终速度统计\n",
    "    if i < 10:\n",
    "        continue\n",
    "    end_time = time.time()\n",
    "    model_time += (end_time - strat_time)\n",
    "avg_model_time = model_time / 90\n",
    "print('self_lstm speed:', avg_model_time, 's')\n",
    "\n",
    "# 计算Paddle内置的SRN运算速度\n",
    "model_time = 0\n",
    "for i in range(100):\n",
    "    strat_time = time.time()\n",
    "    outputs, (hidden_state, cell_state) = paddle_lstm(inputs)\n",
    "    # 预热10次运算，不计入最终速度统计\n",
    "    if i < 10:\n",
    "        continue\n",
    "    end_time = time.time()\n",
    "    model_time += (end_time - strat_time)\n",
    "avg_model_time = model_time / 90\n",
    "print('paddle_lstm speed:', avg_model_time, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以看到，由于Paddle框架的LSTM底层采用了C++实现并进行优化，Paddle框架内置的LSTM运行效率远远高于自己实现的LSTM。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.3.1.2 模型汇总\n",
    "在本节实验中，我们将使用6.1.2.4的Model_RNN4SeqClass作为预测模型，不同在于在实例化时将传入实例化的LSTM层。\n",
    "\n",
    "> 动手联系6.2 在我们手动实现的LSTM算子中，是逐步计算每个时刻的隐状态。请思考如何实现更加高效的LSTM算子。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6.3.2 模型训练\n",
    "#### 6.3.2.1 训练指定长度的数字预测模型\n",
    "本节将基于RunnerV3类进行训练，首先定义模型训练的超参数，并保证和简单循环网络的超参数一致. 然后定义一个`train`函数，其可以通过指定长度的数据集，并进行训练. 在`train`函数中，首先加载长度为`length`的数据，然后实例化各项组件并创建对应的Runner，然后训练该Runner。同时在本节将使用4.5.4节定义的准确度（Accuracy）作为评估指标，代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import paddle\n",
    "import numpy as np\n",
    "from nndl import RunnerV3\n",
    "\n",
    "# 训练轮次\n",
    "num_epochs = 500\n",
    "# 学习率\n",
    "lr = 0.001\n",
    "# 输入数字的类别数\n",
    "num_digits = 10\n",
    "# 将数字映射为向量的维度\n",
    "input_size = 32\n",
    "# 隐状态向量的维度\n",
    "hidden_size = 32\n",
    "# 预测数字的类别数\n",
    "num_classes = 19\n",
    "# 批大小 \n",
    "batch_size = 8\n",
    "# 模型保存目录\n",
    "save_dir = \"./checkpoints\"\n",
    "\n",
    "# 可以设置不同的length进行不同长度数据的预测实验\n",
    "def train(length):\n",
    "    print(f\"\\n====> Training LSTM with data of length {length}.\")\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    paddle.seed(0)\n",
    "\n",
    "    # 加载长度为length的数据\n",
    "    data_path = f\"./datasets/{length}\"\n",
    "    train_examples, dev_examples, test_examples = load_data(data_path)\n",
    "    train_set, dev_set, test_set = DigitSumDataset(train_examples), DigitSumDataset(dev_examples), DigitSumDataset(test_examples)\n",
    "    train_loader = paddle.io.DataLoader(train_set, batch_size=batch_size)\n",
    "    dev_loader = paddle.io.DataLoader(dev_set, batch_size=batch_size)\n",
    "    test_loader = paddle.io.DataLoader(test_set, batch_size=batch_size)\n",
    "    # 实例化模型\n",
    "    base_model = LSTM(input_size, hidden_size)\n",
    "    model = Model_RNN4SeqClass(base_model, num_digits, input_size, hidden_size, num_classes) \n",
    "    # 指定优化器\n",
    "    optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters())\n",
    "    # 定义评价指标\n",
    "    metric = Accuracy()\n",
    "    # 定义损失函数\n",
    "    loss_fn = paddle.nn.CrossEntropyLoss()\n",
    "    # 基于以上组件，实例化Runner\n",
    "    runner = RunnerV3(model, optimizer, loss_fn, metric)\n",
    "\n",
    "    # 进行模型训练\n",
    "    model_save_path = os.path.join(save_dir, f\"best_lstm_model_{length}.pdparams\")\n",
    "    runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=100, log_steps=100, save_path=model_save_path)\n",
    "\n",
    "    return runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.3.2.2 多组训练\n",
    "\n",
    "接下来，分别进行数据长度为10, 15, 20, 25, 30, 35的数字预测模型训练实验，训练后的`runner`保存至`runners`字典中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_runners = {}\n",
    "\n",
    "lengths = [10, 15, 20, 25, 30, 35]\n",
    "for length in lengths:\n",
    "    runner = train(length)\n",
    "    lstm_runners[length] = runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.3.2.3 损失曲线展示\n",
    "分别画出基于LSTM的各个长度的数字预测模型训练过程中，在训练集和验证集上的损失曲线，代码实现如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 画出训练过程中的损失图\n",
    "for length in lengths:\n",
    "    runner = lstm_runners[length]\n",
    "    fig_name = f\"./images/6.11_{length}.pdf\"\n",
    "    plot_training_loss(runner, fig_name, sample_step=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "图6.11展示了LSTM模型在不同长度数据集上进行训练后的损失变化，同SRN模型一样，随着序列长度的增加，训练集上的损失逐渐不稳定，验证集上的损失整体趋向于变大，这说明当序列长度增加时，保持长期依赖的能力同样在逐渐变弱. 同图6.5相比，LSTM模型在序列长度增加时，收敛情况比SRN模型更好。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/f319b75922c74c0c822cb0ab6383a22ca2a8d473ffa9472589e4db90cb70a062\" width=100%></center>\n",
    "<br><center>图6.11 LSTM在不同长度数据集训练损失变化图</center></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> **动手练习6.3**： 改进第6.3.1.1节中的 LSTM 算子，使其可以支持双向 LSTM 模型的计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6.3.3 模型评价\n",
    "\n",
    "#### 6.3.3.1 在测试集上进行模型评价\n",
    "使用测试数据对在训练过程中保存的最好模型进行评价，观察模型在测试集上的准确率. 同时获取模型在训练过程中在验证集上最好的准确率，实现代码如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_dev_scores = []\n",
    "lstm_test_scores = []\n",
    "for length in lengths:\n",
    "    print(f\"Evaluate LSTM with data length {length}.\")\n",
    "    runner = lstm_runners[length]\n",
    "    # 加载训练过程中效果最好的模型\n",
    "    model_path = os.path.join(save_dir, f\"best_lstm_model_{length}.pdparams\")\n",
    "    runner.load_model(model_path)\n",
    "    \n",
    "    # 加载长度为length的数据\n",
    "    data_path = f\"./datasets/{length}\"\n",
    "    train_examples, dev_examples, test_examples = load_data(data_path)\n",
    "    test_set = DigitSumDataset(test_examples)\n",
    "    test_loader = paddle.io.DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "    # 使用测试集评价模型，获取测试集上的预测准确率\n",
    "    score, _ = runner.evaluate(test_loader)\n",
    "    lstm_test_scores.append(score)\n",
    "    lstm_dev_scores.append(max(runner.dev_scores))\n",
    "\n",
    "for length, dev_score, test_score in zip(lengths, lstm_dev_scores, lstm_test_scores):\n",
    "    print(f\"[LSTM] length:{length}, dev_score: {dev_score}, test_score: {test_score: .5f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.3.3.2 模型在不同长度的数据集上的准确率变化图\n",
    "接下来，将SRN和LSTM在不同长度的验证集和测试集数据上的准确率绘制成图片，以方面观察。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lengths, srn_dev_scores, '-o', color='#e4007f',  label=\"SRN Dev Accuracy\")\n",
    "plt.plot(lengths, srn_test_scores,'-o', color='#f19ec2', label=\"SRN Test Accuracy\")\n",
    "plt.plot(lengths, lstm_dev_scores, '-o', color='#e8609b',  label=\"LSTM Dev Accuracy\")\n",
    "plt.plot(lengths, lstm_test_scores,'-o', color='#000000', label=\"LSTM Test Accuracy\")\n",
    "\n",
    "#绘制坐标轴和图例\n",
    "plt.ylabel(\"accuracy\", fontsize='large')\n",
    "plt.xlabel(\"sequence length\", fontsize='large')\n",
    "plt.legend(loc='lower left', fontsize='x-large')\n",
    "\n",
    "fig_name = \"./images/6.12.pdf\"\n",
    "plt.savefig(fig_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "图6.12 展示了LSTM模型与SRN模型在不同长度数据集上的准确度对比。随着数据集长度的增加，LSTM模型在验证集和测试集上的准确率整体也趋向于降低；同时LSTM模型的准确率显著高于SRN模型，表明LSTM模型保持长期依赖的能力要优于SRN模型.\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/2046ae8e1b7d46d4a54da1636d238c647a2292e6c1bf4223b419beb580a1c3cc\" width=50%></center>\n",
    "<br><center>图6.12 LSTM与SRN网络在不同长度数据集上的准确度对比图</center></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> **动手练习6.4**： 请实现 GRU 算子，完成上面实验，并对比 GRU 和 LSTM 的实验效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.3.3.3 LSTM模型门状态和单元状态的变化\n",
    "\n",
    "LSTM模型通过门控机制控制信息的单元状态的更新，这里可以观察当LSTM在处理一条数字序列的时候，相应门和单元状态是如何变化的。首先需要对以上LSTM模型实现代码中，定义相应列表进行存储这些门和单元状态在每个时刻的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn.functional as F\n",
    "# 声明LSTM和相关参数\n",
    "class LSTM(nn.Layer):\n",
    "    def __init__(self, input_size, hidden_size, para_attr=paddle.ParamAttr(initializer=nn.initializer.XavierUniform())):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # 初始化模型参数\n",
    "        self.W_i = paddle.create_parameter(shape=[input_size, hidden_size], dtype=\"float32\", attr=para_attr)\n",
    "        self.W_f = paddle.create_parameter(shape=[input_size, hidden_size], dtype=\"float32\", attr=para_attr)\n",
    "        self.W_o = paddle.create_parameter(shape=[input_size, hidden_size], dtype=\"float32\", attr=para_attr)\n",
    "        self.W_a = paddle.create_parameter(shape=[input_size, hidden_size], dtype=\"float32\", attr=para_attr)\n",
    "        self.U_i = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\", attr=para_attr)\n",
    "        self.U_f = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\", attr=para_attr)\n",
    "        self.U_o = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\", attr=para_attr)\n",
    "        self.U_a = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\", attr=para_attr)\n",
    "        self.b_i = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\", attr=para_attr)\n",
    "        self.b_f = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\", attr=para_attr)\n",
    "        self.b_o = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\", attr=para_attr)\n",
    "        self.b_a = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\", attr=para_attr)\n",
    "\n",
    "    # 初始化状态向量和隐状态向量\n",
    "    def init_state(self, batch_size):\n",
    "        hidden_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=\"float32\")\n",
    "        cell_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=\"float32\")\n",
    "        return hidden_state, cell_state\n",
    "\n",
    "    # 定义前向计算\n",
    "    def forward(self, inputs, states=None):\n",
    "        batch_size, seq_len, input_size = inputs.shape  # inputs batch_size x seq_len x input_size\n",
    "        \n",
    "        if states is None:\n",
    "            states = self.init_state(batch_size)\n",
    "        hidden_state, cell_state = states\n",
    "\n",
    "        # 定义相应的门状态和单元状态向量列表\n",
    "        self.Is = []\n",
    "        self.Fs = []\n",
    "        self.Os = []\n",
    "        self.Cs = []\n",
    "        # 初始化状态向量和隐状态向量\n",
    "        cell_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=\"float32\")\n",
    "        hidden_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=\"float32\")\n",
    "\n",
    "        # 执行LSTM计算，包括：隐藏门、输入门、遗忘门、候选状态向量、状态向量和隐状态向量\n",
    "        for step in range(seq_len):\n",
    "            input_step = inputs[:, step, :]\n",
    "            I_gate = F.sigmoid(paddle.matmul(input_step, self.W_i) + paddle.matmul(hidden_state, self.U_i) + self.b_i)\n",
    "            F_gate = F.sigmoid(paddle.matmul(input_step, self.W_f) + paddle.matmul(hidden_state, self.U_f) + self.b_f)\n",
    "            O_gate = F.sigmoid(paddle.matmul(input_step, self.W_o) + paddle.matmul(hidden_state, self.U_o) + self.b_o)\n",
    "            C_tilde = F.tanh(paddle.matmul(input_step, self.W_a) + paddle.matmul(hidden_state, self.U_a) + self.b_a)\n",
    "            cell_state = F_gate * cell_state + I_gate * C_tilde\n",
    "            hidden_state = O_gate * F.tanh(cell_state)\n",
    "            # 存储门状态向量和单元状态向量\n",
    "            self.Is.append(I_gate.numpy().copy())\n",
    "            self.Fs.append(F_gate.numpy().copy())\n",
    "            self.Os.append(O_gate.numpy().copy())\n",
    "            self.Cs.append(cell_state.numpy().copy())\n",
    "        return hidden_state\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接下来，需要使用新的LSTM模型，重新实例化一个runner，本节使用序列长度为10的模型进行此项实验，因此需要加载序列长度为10的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "base_model = LSTM(input_size, hidden_size)\n",
    "model = Model_RNN4SeqClass(base_model, num_digits, input_size, hidden_size, num_classes) \n",
    "# 指定优化器\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\n",
    "# 定义评价指标\n",
    "metric = Accuracy()\n",
    "# 定义损失函数\n",
    "loss_fn = paddle.nn.CrossEntropyLoss()\n",
    "# 基于以上组件，重新实例化Runner\n",
    "runner = RunnerV3(model, optimizer, loss_fn, metric)\n",
    "\n",
    "length = 10\n",
    "# 加载训练过程中效果最好的模型\n",
    "model_path = os.path.join(save_dir, f\"best_lstm_model_{length}.pdparams\")\n",
    "runner.load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接下来，给定一条数字序列，并使用数字预测模型进行数字预测，这样便会将相应的门状态和单元状态向量保存至模型中. 然后分别从模型中取出这些向量，并将这些向量进行绘制展示。代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_tensor(inputs, tensor,  save_path, vmin=0, vmax=1):\n",
    "    tensor = np.stack(tensor, axis=0)\n",
    "    tensor = np.squeeze(tensor, 1).T\n",
    "\n",
    "    plt.figure(figsize=(16,6))\n",
    "    # vmin, vmax定义了色彩图的上下界\n",
    "    ax = sns.heatmap(tensor, vmin=vmin, vmax=vmax) \n",
    "    ax.set_xticklabels(inputs)\n",
    "    ax.figure.savefig(save_path)\n",
    "\n",
    "\n",
    "# 定义模型输入\n",
    "inputs = [6, 7, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "X = paddle.to_tensor(inputs.copy())\n",
    "X = X.unsqueeze(0)\n",
    "# 进行模型预测，并获取相应的预测结果\n",
    "logits = runner.predict(X)\n",
    "predict_label = paddle.argmax(logits, axis=-1)\n",
    "print(f\"predict result: {predict_label.numpy()[0]}\")\n",
    "\n",
    "# 输入门\n",
    "Is = runner.model.model.Is\n",
    "plot_tensor(inputs, Is, save_path=\"./images/6.13_I.pdf\")\n",
    "# 遗忘门\n",
    "Fs = runner.model.model.Fs\n",
    "plot_tensor(inputs, Fs, save_path=\"./images/6.13_F.pdf\")\n",
    "# 输出门\n",
    "Os = runner.model.model.Os\n",
    "plot_tensor(inputs, Os, save_path=\"./images/6.13_O.pdf\")\n",
    "# 单元状态\n",
    "Cs = runner.model.model.Cs\n",
    "plot_tensor(inputs, Cs, save_path=\"./images/6.13_C.pdf\", vmin=-5, vmax=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "图6.13 当LSTM处理序列数据[6, 7, 0, 0, 1, 0, 0, 0, 0, 0]的过程中单元状态和门数值的变化图，其中横坐标为输入数字，纵坐标为相应门或单元状态向量的维度，颜色的深浅代表数值的大小。可以看到，当输入门遇到不同位置的数字0时，保持了相对一致的数值大小，表明对于0元素保持相同的门控过滤机制，避免输入信息的变化给当前模型带来困扰；当遗忘门遇到数字1后，遗忘门数值在一些维度上变小，表明对某些信息进行了遗忘；随着序列的输入，输出门和单元状态在某些维度上数值变小，在某些维度上数值变大，表明输出门在根据信息的重要性选择信息进行输出，同时单元状态也在保持着对文本预测重要的一些信息.\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/392882d8639547e18bbed77ac05dc7670df069940210496f9a12b13b207c162a\" width=100%></center>\n",
    "<br><center>图6.13 LSTM中单元状态和门数值的变化图</center></br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
